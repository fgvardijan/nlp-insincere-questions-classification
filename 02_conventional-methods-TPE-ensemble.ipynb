{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "710ed17d0c57bd287be0ee3b2782a53a54510561"
   },
   "source": [
    "## Conventional Text Preprocessing with Bag of Words, Machine Learning + Bayesian Hyperparameters Tuning with TPE and Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "585eec8b444e194eda5a141d2a2316547235ffdf"
   },
   "source": [
    "In this kernel, i will clean the text and use normal conventional methods to prepare the text for machine learning algorithms:\n",
    "- CountVectorizer\n",
    "- TFIDF\n",
    "\n",
    "After input is in format suitable for machine learning alorithms, I will train few classical ML algorithms with default parameters:\n",
    "- LogisticRegression\n",
    "- Naive Bayes\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "Next, I will try to optimize hyperparameters for each model using Bayesian Optimization with Tree Parzen Estimator (TPE) algorithm.\n",
    "\n",
    "Finally, optimized models will be ensebmled by averaging predictions and forming majority rule ensemble.\n",
    "\n",
    "References:\n",
    "* https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n",
    "* https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "abb7e3c30b8a412a50c6b451c49939e3cf4bc11b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os \n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import  SnowballStemmer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a4ff5590a6f152dc1bec5aeca79aef10218f7de"
   },
   "source": [
    "### Basic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "deee49df5ca1c4413f71677939e26aa1ff784e44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEED = 2576"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea10c8e218a1280faa9802bcb7f1117c89ec96f9"
   },
   "source": [
    "## Data Preparation and Cleaning\n",
    "\n",
    "Basic Preprocessing Techniques for text data:\n",
    "* Cleaning Special Characters and Removing Punctuations\n",
    "* Cleaning Numbers\n",
    "* Removing Misspells\n",
    "* Removing Contractions\n",
    "\n",
    "Since we are going to create features for words in the feature creation step, it makes sense to reduce words to a common denominator so that ‘organize’, ‘organizes’ and ‘organizing’ could be referred to by a single word ‘organize’.\n",
    "\n",
    "There are few most common ways to do this:\n",
    "* Stemming\n",
    "    * Stemming is the process of converting words to their base forms using crude Heuristic rules. For example, one rule could be to remove ’s’ from the end of any word, so that ‘cats’ becomes ‘cat’. or another rule could be to replace ‘ies’ with ‘i’ so that ‘ponies becomes ‘poni’.\n",
    "* Lemmatization \n",
    "    * Lemmatization is very similar to stemming but it aims to remove endings only if the base form is present in a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "abeab4c80d6829cf2eae706bfa7929e2871af81f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This preprocesssing is common to most text classification methods.\n",
    "\n",
    "# remove punctuations:\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' ')\n",
    "    return x\n",
    "\n",
    "# We won't clean numbers in conventional methods case since we might get extra info \n",
    "# from bigrams like 5 mins or 30 mins\n",
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d', x)):\n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]{4}', '####', x)\n",
    "        x = re.sub('[0-9]{3}', '###', x)\n",
    "        x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "# remove Misspell:\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "# remove stopwords:\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text, is_lower_case=True):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# remove contractions:\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# use Stemming to convert words to their base form\n",
    "def stem_text(text):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(tokens)\\\n",
    "\n",
    "\n",
    "# use Lemmatization to keep dictionary form of words. Might be helpful if later we want to use word embeddings.\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemma_text(text):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def clean_sentence(x):\n",
    "    x = x.lower()\n",
    "    x = clean_text(x)\n",
    "    x = replace_typical_misspell(x)\n",
    "    x = remove_stopwords(x)\n",
    "    x = replace_contractions(x)\n",
    "    x = lemma_text(x)\n",
    "    x = x.replace(\"'\",\"\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "63cb21525251b060aeb309e7be4b48772f8720f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (100000, 3)\n",
      "Test shape :  (100000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")[:100000]\n",
    "test_df = pd.read_csv(\"../input/test.csv\")[:100000]\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "2b23f5bcfd9bd3d47652ac34454f842ae7f6726a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4cd4bd5b2aa754993e4c4fa520f5d62d3f99f034"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aedd3a83d7b4a55b31699350d4de155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209c9f50831045fcb15f0efe083e6691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clean the sentences\n",
    "train_df['cleaned_text'] = train_df['question_text'].progress_apply(lambda x : clean_sentence(x))\n",
    "test_df['cleaned_text'] = test_df['question_text'].progress_apply(lambda x : clean_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>quebec nationalist see province nation 1960s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>adopted dog would encourage people adopt shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "      <td>velocity affect time velocity affect space geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "      <td>otto von guericke used magdeburg hemisphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "      <td>convert montra helicon mountain bike changing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target                                       cleaned_text  \n",
       "0       0       quebec nationalist see province nation 1960s  \n",
       "1       0      adopted dog would encourage people adopt shop  \n",
       "2       0  velocity affect time velocity affect space geo...  \n",
       "3       0        otto von guericke used magdeburg hemisphere  \n",
       "4       0  convert montra helicon mountain bike changing ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text representaion for conventional ML\n",
    "To use text in conventional machine learning models we need to create features from word and features need to be represented in vector format.\n",
    "\n",
    "Some of representations that achieve that are:\n",
    "\n",
    "__Bag of Words - Countvectorizer Features__\n",
    "\n",
    "All words in corpus are encoded with a dictionary. Every sentece is then represented with frequency of the words appearing in sentence.\n",
    "\n",
    "__TFIDF Features__\n",
    "\n",
    "Similar to Countvectorizer but with TFIDF we take features only for the significant words. Important factors here are:\n",
    "* Term Frequency: How important is the word in the document?\n",
    "* Inverse Document Frequency: How important the term is in the whole corpus?\n",
    "\n",
    "TFIDF then is just multiplication of these two scores. This techinique allowes to find important words in a document which are also not very common.\n",
    "\n",
    "__Hashing Features__\n",
    "\n",
    "This technique uses hashes to redunce the vocabulary size. We won't explore it here.\n",
    "\n",
    "__Word2vec Features__\n",
    "\n",
    "This techique uses embeddings to represent each word in an \"n-dimensional space\". This techinque is very powerfull when used with neural networks with Embedding layer. We won't explore it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b785fad987047ecc499fea7557c7e24e0deae872"
   },
   "source": [
    "#### Bag of words model using Count Vectorizer\n",
    "We will use the simplest method \"Bag of Words\" to represent text as vectors before using it with machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f717429c36d904902fc406edf7a820943261950e"
   },
   "source": [
    "![count vectorizer](../images/bag_of_words.png \"Bag of Words\")\n",
    "\n",
    "Important parameters:\n",
    "* `ngram_range`: we use (1,3). This means that unigrams, bigrams, and trigrams will be taken into account while creating features.\n",
    "* `min_df`: Minimum number of times an ngram should appear in a corpus to be used as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "c0889276092d61b1146c2561b52da39448755bdd"
   },
   "outputs": [],
   "source": [
    "cnt_vectorizer = CountVectorizer(dtype=np.float32,\n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3),min_df=3)\n",
    "\n",
    "# Fitting count vectorizer to both training and test sets (semi-supervised learning)\n",
    "cnt_vectorizer.fit(list(train_df.cleaned_text.values) + list(test_df.cleaned_text.values))\n",
    "\n",
    "xtrain =  cnt_vectorizer.transform(train_df.cleaned_text.values) \n",
    "y_train = train_df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 79652)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d0f7bd7abed7262a7c09c2b6bd16665529c60b1"
   },
   "source": [
    "## Creating Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "5203c177aa11fe6eadccadcd91ae2c8c7d5ef661"
   },
   "outputs": [],
   "source": [
    "# helper function to find threshold and find best f score - Eval metric of competition\n",
    "def bestThresshold(y_train,train_preds):\n",
    "    tmp = [0,0,0] # idx, cur, max\n",
    "    delta = 0\n",
    "    for tmp[0] in np.arange(0.1, 0.501, 0.01):\n",
    "        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n",
    "        if tmp[1] > tmp[2]:\n",
    "            delta = tmp[0]\n",
    "            tmp[2] = tmp[1]\n",
    "    # print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "    return tmp[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "bce223df048847b1cdb7d8f04ea7c686350be744"
   },
   "outputs": [],
   "source": [
    "# helper function to train model with cross validation and get out of fold predictions on the test set\n",
    "def model_train_cv(x_train,y_train,nfold,model_obj):\n",
    "    splits = list(StratifiedKFold(n_splits=nfold, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "    x_train = x_train\n",
    "    y_train = np.array(y_train)\n",
    "    # matrix for the out-of-fold predictions\n",
    "    train_oof_preds = np.zeros((x_train.shape[0]))\n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "\n",
    "        x_train_fold = x_train[train_idx.astype(int)]\n",
    "        y_train_fold = y_train[train_idx.astype(int)]\n",
    "        x_val_fold = x_train[valid_idx.astype(int)]\n",
    "        y_val_fold = y_train[valid_idx.astype(int)]\n",
    "\n",
    "        clf = copy.deepcopy(model_obj)\n",
    "        clf.fit(x_train_fold, y_train_fold)\n",
    "        valid_preds_fold = clf.predict_proba(x_val_fold)[:,1]\n",
    "\n",
    "        # storing OOF predictions\n",
    "        train_oof_preds[valid_idx] = valid_preds_fold\n",
    "    return train_oof_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Bayesian Optimization Method\n",
    "Bayesian method will be use used to tune the model hyperparameters. \n",
    "\n",
    "Bayesian methods differ from random or grid search in that they use past evaluation results to choose the next values to evaluate. The concept is: limit expensive evaluations of the objective function by choosing the next input values based on those that have done well in the past.\n",
    "\n",
    "Main parts of Bayesian Optimization problem are:\n",
    "1. __Objective Function__: what we want to minimize, in this case the validation error of a machine learning model with respect to the hyperparameters\n",
    "1. __Domain Space__: hyperparameter values to search over\n",
    "1. __Optimization algorithm__: method for constructing the surrogate model and choosing the next hyperparameter values to evaluate\n",
    "1. __Result history__: stored outcomes from evaluations of the objective function consisting of the hyperparameters and validation loss\n",
    "\n",
    "With those four pieces, we can optimize (find the minimum) of any function that returns a real value. \n",
    "\n",
    "In this notebook, `Hyperopt` library with Tree Parzen Estimator (TPE) as optimization algorithm will be used.\n",
    "\n",
    "> Note: number of iterations for choosing hyperparamters will be limited 10 or less due to low capacity of on my PC, so results will be random and not optimal. If we could let the optimiztion algorithm to run longer, e.g. 500 times, we would see convergence to optimal parmeters.\n",
    "\n",
    "Reference: https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "fbb4b1e370b6ef71a2c79883f5084b62f1d6d31a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.549 \n"
     ]
    }
   ],
   "source": [
    "train_oof_preds = model_train_cv(xtrain,y_train,5,LogisticRegression(C=1.0))\n",
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "620b177a5db5cb39ac36e648fdcadcf941e6758d"
   },
   "source": [
    "We are able to get an F1 local CV score of 0.549 with default model which just counts the number of time some ngrams appear in a sentence. Let's try tuning the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter tuning for a simple Logistic Regression with Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll tune the logistic regression classsifier, using hyperopt library. The hyperopt library has a similar purpose as gridsearch, but instead of doing an exhaustive search of the parameter space it evaluates a few well-chosen data points and then extrapolates the optimal solution based on modeling. In practice that means it often needs much fewer iterations to find a good solution.\n",
    "\n",
    "The important parameters to tune are:\n",
    "* `C` - inverse regularization strength. \n",
    "    * For small values of C, we increase the regularization strength which will create simple models which underfit the data. \n",
    "    * For big values of C, we low the power of regularization which imples the model is allowed to increase it's complexity, and therefore, overfit the data \n",
    "* `solver` - algorithm to use in the optimization problem, since we have every large, sparse dataset we'll use Stochastic Average Gradient variant `saga`\n",
    "* `penalty` - norm used in the penalization, but we'll use only `l2` as it is suported by all solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "5fd52781-cf9a-4299-8d35-93800e90a36d",
    "_uuid": "958d1109036581199cea00cd75cd7451a6f48f7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {'C': params['C']}        \n",
    "    clf = LogisticRegression(\n",
    "        solver='saga', \n",
    "        max_iter=1000,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    n_folds = 5\n",
    "    train_oof_preds = model_train_cv(xtrain,y_train,n_folds,clf)\n",
    "    \n",
    "    score = bestThresshold(y_train,train_oof_preds)\n",
    "    loss = 1 - score  # objective function returns loss to minimize\n",
    "    print(\"Loss {:.3f} params {}\".format(loss, params))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "5fd52781-cf9a-4299-8d35-93800e90a36d",
    "_uuid": "958d1109036581199cea00cd75cd7451a6f48f7d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 0/10 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.474 params {'C': 9.653515323349497}\n",
      " 10%|█████▏                                              | 1/10 [03:39<32:52, 219.17s/it, best loss: 0.473708629755645]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.463 params {'C': 2.7794458879762054}\n",
      "Loss 0.450 params {'C': 0.974347331723907}\n",
      "Loss 0.483 params {'C': 0.0427166098571098}\n",
      " 40%|████████████████████                              | 4/10 [09:33<14:11, 141.83s/it, best loss: 0.45036282229427205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.694 params {'C': 0.0010721186361638802}\n",
      "Loss 0.490 params {'C': 0.03240995351294748}\n",
      "Loss 0.446 params {'C': 0.3376656942908817}\n",
      "Loss 0.527 params {'C': 0.01246817925733405}\n",
      "Loss 0.456 params {'C': 0.131346235470916}\n",
      "Loss 0.450 params {'C': 0.9737908722066724}\n",
      "100%|██████████████████████████████████████████████████| 10/10 [15:17<00:00, 75.63s/it, best loss: 0.44610820737392276]\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'C': hp.loguniform('C', low=-3*np.log(10), high=np.log(10))  # C [0.001, 10]\n",
    "}\n",
    "\n",
    "best_logit = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10,\n",
    "            rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit, best F1-score 0.554 \n",
      "Logit, best parameters chosen with Hyperopt are: {'C': 0.3376656942908817}\n"
     ]
    }
   ],
   "source": [
    "print('Logit, best F1-score %0.3f ' % (1-0.44610820737392276))\n",
    "print('Logit, best parameters chosen with Hyperopt are: '+str(best_logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Logit, best F1-score 0.554 \n",
    "Logit, best parameters chosen with Hyperopt are: {'C': 0.3376656942908817}`\n",
    "\n",
    "So, with hyperparameter tuning we've found better value for C=0.34, which had improved F1-score comapred to default model (C=1). We should explore this further by naroving down distribtuion of C to to have only values between (0.2, 1.0) as samples from that range yielded best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a simple Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ab05c54baa2d43afdd848c346bb1453977755cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.498 \n"
     ]
    }
   ],
   "source": [
    "train_oof_preds = model_train_cv(xtrain,y_train,5,MultinomialNB())\n",
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ff5c6c21fad9b7b849791bdd42d1685d3fe865a"
   },
   "source": [
    "We are able to get an F1 local CV score of 0.498 with default model. Let's try tuning the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter tuning for a simple Naive Bayes with Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "5fd52781-cf9a-4299-8d35-93800e90a36d",
    "_uuid": "958d1109036581199cea00cd75cd7451a6f48f7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {'alpha': params['alpha']}        \n",
    "    clf = MultinomialNB(**params)\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    n_folds = 5\n",
    "    train_oof_preds = model_train_cv(xtrain,y_train,n_folds,clf)\n",
    "    \n",
    "    score = bestThresshold(y_train,train_oof_preds)\n",
    "    loss = 1 - score  # objective function returns loss to minimize\n",
    "    print(\"Loss {:.3f} params {}\".format(loss, params))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "5fd52781-cf9a-4299-8d35-93800e90a36d",
    "_uuid": "958d1109036581199cea00cd75cd7451a6f48f7d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.528 params {'alpha': 1.496171372572428}\n",
      "Loss 0.521 params {'alpha': 1.360989555833787}\n",
      "Loss 0.515 params {'alpha': 1.2471784500414422}\n",
      "Loss 0.499 params {'alpha': 0.907649194556314}\n",
      "Loss 0.498 params {'alpha': 0.5075607113066694}\n",
      "Loss 0.497 params {'alpha': 0.8776696020261421}\n",
      "Loss 0.509 params {'alpha': 1.1321217350011867}\n",
      "Loss 0.494 params {'alpha': 0.7739507593955738}\n",
      "Loss 0.504 params {'alpha': 1.0296044074844866}\n",
      "Loss 0.515 params {'alpha': 1.2471164248463555}\n",
      "Loss 0.506 params {'alpha': 1.059792010384872}\n",
      "Loss 0.518 params {'alpha': 1.3071413385798567}\n",
      "Loss 0.495 params {'alpha': 0.6012325414277948}\n",
      "Loss 0.509 params {'alpha': 1.1417322312889917}\n",
      "Loss 0.524 params {'alpha': 1.4265514535647026}\n",
      "Loss 0.495 params {'alpha': 0.8250262881583786}\n",
      "Loss 0.498 params {'alpha': 0.5558786294367265}\n",
      "Loss 0.494 params {'alpha': 0.6667158345220777}\n",
      "Loss 0.505 params {'alpha': 1.036303350575253}\n",
      "Loss 0.527 params {'alpha': 1.4758255838270777}\n",
      "Loss 0.494 params {'alpha': 0.7124782276197604}\n",
      "Loss 0.494 params {'alpha': 0.7219104621444755}\n",
      "Loss 0.494 params {'alpha': 0.6739911623384737}\n",
      "Loss 0.494 params {'alpha': 0.7412127842180694}\n",
      "Loss 0.500 params {'alpha': 0.941703939930916}\n",
      "Loss 0.494 params {'alpha': 0.6488308404634955}\n",
      "Loss 0.494 params {'alpha': 0.762395229690172}\n",
      "Loss 0.498 params {'alpha': 0.5008665007735158}\n",
      "Loss 0.495 params {'alpha': 0.8177027339454932}\n",
      "Loss 0.500 params {'alpha': 0.9592334981133301}\n",
      "Loss 0.494 params {'alpha': 0.711135601710108}\n",
      "Loss 0.494 params {'alpha': 0.6241951039813362}\n",
      "Loss 0.496 params {'alpha': 0.5819761033111714}\n",
      "Loss 0.499 params {'alpha': 0.5339328650617546}\n",
      "Loss 0.496 params {'alpha': 0.8491127278951401}\n",
      "Loss 0.498 params {'alpha': 0.9032702342987251}\n",
      "Loss 0.508 params {'alpha': 1.1031674921622612}\n",
      "Loss 0.494 params {'alpha': 0.7050430806749753}\n",
      "Loss 0.494 params {'alpha': 0.8048055789373882}\n",
      "Loss 0.513 params {'alpha': 1.2022311844963878}\n",
      "Loss 0.500 params {'alpha': 0.9711996816638073}\n",
      "Loss 0.496 params {'alpha': 0.8609754499671072}\n",
      "Loss 0.499 params {'alpha': 0.9140808252668711}\n",
      "Loss 0.502 params {'alpha': 1.0024263488952623}\n",
      "Loss 0.494 params {'alpha': 0.7732668582874501}\n",
      "Loss 0.498 params {'alpha': 0.507133066620749}\n",
      "Loss 0.497 params {'alpha': 0.5750609094103107}\n",
      "Loss 0.494 params {'alpha': 0.7291269363145303}\n",
      "Loss 0.507 params {'alpha': 1.0752833237494102}\n",
      "Loss 0.494 params {'alpha': 0.6325116489386312}\n",
      "Loss 0.521 params {'alpha': 1.363994619940986}\n",
      "Loss 0.511 params {'alpha': 1.1703527939332268}\n",
      "Loss 0.498 params {'alpha': 0.5480209959561703}\n",
      "Loss 0.494 params {'alpha': 0.6828800320742531}\n",
      "Loss 0.494 params {'alpha': 0.7968751498322056}\n",
      "Loss 0.498 params {'alpha': 0.8981113516802823}\n",
      "Loss 0.518 params {'alpha': 1.2988695744736793}\n",
      "Loss 0.495 params {'alpha': 0.6040150083908191}\n",
      "Loss 0.496 params {'alpha': 0.8614992814231951}\n",
      "Loss 0.502 params {'alpha': 1.0100716245324826}\n",
      "Loss 0.494 params {'alpha': 0.7461484674488195}\n",
      "Loss 0.494 params {'alpha': 0.6722727646942726}\n",
      "Loss 0.500 params {'alpha': 0.9324187958464422}\n",
      "Loss 0.499 params {'alpha': 0.5345950139289164}\n",
      "Loss 0.506 params {'alpha': 1.059627676732527}\n",
      "Loss 0.494 params {'alpha': 0.7206555279083581}\n",
      "Loss 0.494 params {'alpha': 0.7050612504285039}\n",
      "Loss 0.495 params {'alpha': 0.8423482718197002}\n",
      "Loss 0.494 params {'alpha': 0.6233438840070096}\n",
      "Loss 0.494 params {'alpha': 0.7854364449223847}\n",
      "Loss 0.495 params {'alpha': 0.8248631258571395}\n",
      "Loss 0.501 params {'alpha': 0.9794107554133984}\n",
      "Loss 0.494 params {'alpha': 0.6505293468868429}\n",
      "Loss 0.496 params {'alpha': 0.8757686444630044}\n",
      "Loss 0.496 params {'alpha': 0.5794852163966546}\n",
      "Loss 0.494 params {'alpha': 0.7557131850066793}\n",
      "Loss 0.494 params {'alpha': 0.7555652253604723}\n",
      "Loss 0.494 params {'alpha': 0.7432529870150381}\n",
      "Loss 0.500 params {'alpha': 0.9517886700595195}\n",
      "Loss 0.495 params {'alpha': 0.6032413808437629}\n",
      "Loss 0.494 params {'alpha': 0.7772915577582469}\n",
      "Loss 0.499 params {'alpha': 0.9230829729624248}\n",
      "Loss 0.494 params {'alpha': 0.6870691067056419}\n",
      "Loss 0.509 params {'alpha': 1.1288606330795965}\n",
      "Loss 0.494 params {'alpha': 0.7539063076271606}\n",
      "Loss 0.494 params {'alpha': 0.6498241560440827}\n",
      "Loss 0.495 params {'alpha': 0.8071669831342199}\n",
      "Loss 0.498 params {'alpha': 0.8944606822060628}\n",
      "Loss 0.504 params {'alpha': 1.0365887566076653}\n",
      "Loss 0.495 params {'alpha': 0.8329664530150589}\n",
      "Loss 0.527 params {'alpha': 1.478907228277264}\n",
      "Loss 0.499 params {'alpha': 0.5229266832553676}\n",
      "Loss 0.494 params {'alpha': 0.6240449663402002}\n",
      "Loss 0.497 params {'alpha': 0.5620651117458143}\n",
      "Loss 0.513 params {'alpha': 1.2114854115334033}\n",
      "Loss 0.496 params {'alpha': 0.8702960300517565}\n",
      "Loss 0.494 params {'alpha': 0.6906000172191825}\n",
      "Loss 0.508 params {'alpha': 1.0954229930416663}\n",
      "Loss 0.501 params {'alpha': 0.981273178888308}\n",
      "Loss 0.494 params {'alpha': 0.7648290071528995}\n",
      "100%|████████████████████████████████████████████████| 100/100 [01:11<00:00,  1.43it/s, best loss: 0.49354675895882905]\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'alpha': hp.uniform('alpha', 0.5, 1.5)\n",
    "}\n",
    "\n",
    "best_nb = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, best F1-score 0.506 \n",
      "Naive Bayes, best parameters chosen with Hyperopt are: {'alpha': 0.7124782276197604}\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes, best F1-score %0.3f ' % (1-0.49354675895882905))\n",
    "print('Naive Bayes, best parameters chosen with Hyperopt are: '+str(best_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Naive Bayes, best F1-score 0.506 \n",
    "Naive Bayes, best parameters chosen with Hyperopt are: {'alpha': 0.7124782276197604}`\n",
    "\n",
    "With hyperparameter tuning we've found better value for alpha=0.71, which had improved F1-score comapred to default model (alpha=1). \n",
    "\n",
    "> Note: Since this is fast model to train, over 100 iteration we see how TPE algorithm stars to exploit the distribution of alpha where the lowest loss is achieved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Random Forest Model\n",
    "Random forests work by averaging predictions from many decision trees - the idea is that by averaging many trees the mistakes of each tree are ironed out. Each decision tree can be somewhat overfitted, by averaging them the final result should be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "ab05c54baa2d43afdd848c346bb1453977755cec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\efilgva\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3.6-tensorflow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.426 \n"
     ]
    }
   ],
   "source": [
    "train_oof_preds = model_train_cv(xtrain,y_train,5,RandomForestClassifier(n_jobs=4, class_weight='balanced'))\n",
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ff5c6c21fad9b7b849791bdd42d1685d3fe865a"
   },
   "source": [
    "We got a much lower F1 on local CV score with default model compared to simpler models above. \n",
    "> Training of Random Forest takes a lot of time.\n",
    "\n",
    "Let's try tuning the parameters but with low number of evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter tuning for a Random Forest with Hyperopt\n",
    "\n",
    "The important parameters to tune are:\n",
    "\n",
    "* Number of trees in the forest (n_estimators)\n",
    "* Tree complexity (max_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "5fd52781-cf9a-4299-8d35-93800e90a36d",
    "_uuid": "958d1109036581199cea00cd75cd7451a6f48f7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "              'max_depth': int(params['max_depth'])} \n",
    "    clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', **params)\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    n_folds = 2\n",
    "    train_oof_preds = model_train_cv(xtrain,y_train,n_folds,clf)\n",
    "    \n",
    "    score = bestThresshold(y_train,train_oof_preds)\n",
    "    loss = 1 - score  # objective function returns loss to minimize\n",
    "    print(\"Loss {:.3f} params {}\".format(loss, params))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "5fd52781-cf9a-4299-8d35-93800e90a36d",
    "_uuid": "958d1109036581199cea00cd75cd7451a6f48f7d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.554 params {'n_estimators': 500, 'max_depth': 6}\n",
      "Loss 0.554 params {'n_estimators': 425, 'max_depth': 5}\n",
      "Loss 0.552 params {'n_estimators': 375, 'max_depth': 10}\n",
      "100%|█████████████████████████████████████████████████████| 3/3 [00:53<00:00, 18.22s/it, best loss: 0.5521843275921068]\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 25, 500, 25),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 10, 1),\n",
    "}\n",
    "\n",
    "best_rf = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=3,\n",
    "            rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest, best parameters chosen with Hyperopt are: {'max_depth': 10.0, 'n_estimators': 375.0}\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest, best parameters chosen with Hyperopt are: '+str(best_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Random Forest, best parameters chosen with Hyperopt are: {'n_estimators': 375, 'max_depth': 10}`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a XGBoost Model\n",
    "XGBoost is also an based on an ensemble of decision trees, but different from random forest. The trees are not averaged, but added. The decision trees are trained to correct residuals from the previous trees. The idea is that many small decision trees are trained, each adding a bit of info to improve overall predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "ab05c54baa2d43afdd848c346bb1453977755cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.482 \n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        learning_rate=0.05,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "train_oof_preds = model_train_cv(xtrain,y_train,5,clf)\n",
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ff5c6c21fad9b7b849791bdd42d1685d3fe865a"
   },
   "source": [
    "We've also got a much lower F1 on local CV score with default model compared to simpler models above. But at least better and faster than Random Forest.\n",
    "\n",
    "Let's try tuning the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter tuning for a XGBoost with Hyperopt\n",
    "\n",
    "The most important parameters are:\n",
    "* Number of trees (n_estimators)\n",
    "* Learning rate - later trees have less influence (learning_rate)\n",
    "* Tree complexity (max_depth)\n",
    "* Gamma - Make individual trees conservative, reduce overfitting \n",
    "* Column sample per tree - reduce overfitting\n",
    "\n",
    "We will fix the number of trees to 250 and learning rate to 0.05 - then we can find good values for the other parameters. Later we can re-visit this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "5fd52781-cf9a-4299-8d35-93800e90a36d",
    "_uuid": "958d1109036581199cea00cd75cd7451a6f48f7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "    }\n",
    "    \n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        learning_rate=0.05,\n",
    "        n_jobs=4,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    n_folds = 5\n",
    "    train_oof_preds = model_train_cv(xtrain,y_train,n_folds,clf)\n",
    "    \n",
    "    score = bestThresshold(y_train,train_oof_preds)\n",
    "    loss = 1 - score  # objective function returns loss to minimize\n",
    "    print(\"Loss {:.3f} params {}\".format(loss, params))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "5fd52781-cf9a-4299-8d35-93800e90a36d",
    "_uuid": "958d1109036581199cea00cd75cd7451a6f48f7d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.485 params {'max_depth': 8, 'gamma': '0.292', 'colsample_bytree': '0.719'}\n",
      "Loss 0.489 params {'max_depth': 7, 'gamma': '0.226', 'colsample_bytree': '0.650'}\n",
      "Loss 0.493 params {'max_depth': 6, 'gamma': '0.492', 'colsample_bytree': '0.921'}\n",
      "Loss 0.509 params {'max_depth': 4, 'gamma': '0.031', 'colsample_bytree': '0.635'}\n",
      "Loss 0.537 params {'max_depth': 2, 'gamma': '0.447', 'colsample_bytree': '0.447'}\n",
      "100%|████████████████████████████████████████████████████| 5/5 [02:18<00:00, 25.81s/it, best loss: 0.48458817584638714]\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0)\n",
    "}\n",
    "\n",
    "best_xgb = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=5,\n",
    "            rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost, best parameters chosen with Hyperopt are: {'colsample_bytree': 0.7189583460013607, 'gamma': 0.2922725910062163, 'max_depth': 8.0}\n"
     ]
    }
   ],
   "source": [
    "print('XGBoost, best parameters chosen with Hyperopt are: '+str(best_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`XGBoost, best parameters chosen with Hyperopt are: {'colsample_bytree': 0.7189583460013607, 'gamma': 0.2922725910062163, 'max_depth': 8.0}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a LightGBM Model\n",
    "LightGBM is very similar to xgboost, it is also uses a gradient boosted tree approach. So the explanation above mostly holds also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "train_oof_preds = model_train_cv(xtrain,y_train,5,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.522 \n"
     ]
    }
   ],
   "source": [
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_train,train_oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter tuning for a LightGBM with Hyperopt\n",
    "\n",
    "The important parameters to tune are:\n",
    "* Number of estimators\n",
    "* Tree complexity - in lightgbm that is controlled by number of leaves (num_leaves)\n",
    "* Learning rate\n",
    "* Feature fraction\n",
    "\n",
    "We will fix number of estimators to 500 and learning rate to 0.01 (chosen experimentally) and tune the remaining parameters with hyperopt. Then later we could revisit for better results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "5fd52781-cf9a-4299-8d35-93800e90a36d",
    "_uuid": "958d1109036581199cea00cd75cd7451a6f48f7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'num_leaves': int(params['num_leaves']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "    }\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    n_folds = 5\n",
    "    train_oof_preds = model_train_cv(xtrain,y_train,n_folds,clf)\n",
    "    \n",
    "    score = bestThresshold(y_train,train_oof_preds)\n",
    "    loss = 1 - score  # objective function returns loss to minimize\n",
    "    print(\"Loss {:.3f} params {}\".format(loss, params))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.464 params {'num_leaves': 112, 'colsample_bytree': '0.893'}\n",
      "Loss 0.471 params {'num_leaves': 38, 'colsample_bytree': '0.541'}\n",
      "Loss 0.465 params {'num_leaves': 118, 'colsample_bytree': '0.912'}\n",
      "Loss 0.462 params {'num_leaves': 124, 'colsample_bytree': '0.617'}\n",
      "Loss 0.459 params {'num_leaves': 106, 'colsample_bytree': '0.468'}\n",
      "100%|████████████████████████████████████████████████████| 5/5 [11:24<00:00, 144.31s/it, best loss: 0.4586096416064377]\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'num_leaves': hp.quniform('num_leaves', 8, 128, 2),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "}\n",
    "\n",
    "best_lgm = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM, best parameters chosen with Hyperopt are: {'colsample_bytree': 0.46753405887357136, 'num_leaves': 106.0}\n"
     ]
    }
   ],
   "source": [
    "print('LightGBM, best parameters chosen with Hyperopt are: '+str(best_lgm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LightGBM, best parameters chosen with Hyperopt are: {'colsample_bytree': 0.30915021459799613, 'num_leaves': 88.0}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a54bc925-565d-4149-a5d5-6a0d68c526b7",
    "_uuid": "7928486fcc1ec21967c768456b9629cc97938657"
   },
   "source": [
    "## Comparing the models\n",
    "\n",
    "Now let's see how the models perform - if hyperopt has determined a sensible set of parameters for us..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = LogisticRegression(\n",
    "    C=1.0,\n",
    "    solver='saga',\n",
    "    max_iter=4000\n",
    ")\n",
    "\n",
    "nb_model = MultinomialNB(\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_jobs=4,\n",
    "    class_weight='balanced',\n",
    "    n_estimators=325,\n",
    "    max_depth=7\n",
    ")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=250,\n",
    "    learning_rate=0.05,\n",
    "    n_jobs=4,\n",
    "    max_depth=8,\n",
    "    colsample_bytree=0.7,\n",
    "    gamma=0.3\n",
    ")\n",
    "\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=88,\n",
    "    colsample_bytree=0.3\n",
    ")\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', logit_model),\n",
    "    ('Naive Bayes', nb_model),\n",
    "    ('Random Forest', rf_model),\n",
    "    ('XGBoost', xgb_model),\n",
    "    ('LightGBM', lgbm_model)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.5497 [Logistic Regression]\n",
      "F1-Score: 0.5065 [Naive Bayes]\n",
      "F1-Score: 0.4367 [Random Forest]\n",
      "F1-Score: 0.5147 [XGBoost]\n",
      "F1-Score: 0.5390 [LightGBM]\n"
     ]
    }
   ],
   "source": [
    "for label, model in models:\n",
    "    train_oof_preds = model_train_cv(xtrain,y_train,5,model)\n",
    "    score = bestThresshold(y_train,train_oof_preds)\n",
    "    print(\"F1-Score: %0.4f [%s]\" % (score, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for hyperparameter tuning. We've got following perfomances from tuned models:\n",
    "* F1-Score: 0.5497 [Logistic Regression]\n",
    "* F1-Score: 0.5065 [Naive Bayes]\n",
    "* F1-Score: 0.4367 [Random Forest]\n",
    "* F1-Score: 0.5147 [XGBoost]\n",
    "* F1-Score: 0.5390 [LightGBM]\n",
    "\n",
    "It is important to note that when using Bayesian methods, high number of evaluations are needed to find best paramteres. Advantage is that Bayesian method will start exploiting values for best paramteres which we can use to narrow down the search range and converge faster than with Random Search or Grid Search/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10c39195707e4710e11d31bafd4bde1e21ff2d6f"
   },
   "source": [
    "References:\n",
    "    https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a54bc925-565d-4149-a5d5-6a0d68c526b7",
    "_uuid": "7928486fcc1ec21967c768456b9629cc97938657"
   },
   "source": [
    "## Making predictions from ensemble\n",
    "Here I will try to make predictions from three previously trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 79652) (90000,) (10000, 79652) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# split data to train and validation sets\n",
    "X_t, X_v, y_t, y_v = train_test_split(\n",
    "                                    xtrain, \n",
    "                                    y_train,\n",
    "                                    test_size=0.1, \n",
    "                                    random_state=42, \n",
    "                                    stratify=y_train)\n",
    "print(X_t.shape, \n",
    "      y_t.shape, \n",
    "      X_v.shape, \n",
    "      y_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get better result with ensembling 3 models which work in very different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "        importance_type='split', learning_rate=0.01, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=500, n_jobs=-1, num_leaves=88, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "logit_model.fit(X_t, y_t)\n",
    "# Naive Bayes\n",
    "nb_model.fit(X_t, y_t)\n",
    "# LightGBM\n",
    "lgbm_model.fit(X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on hold-out set\n",
    "pred_val_1 = logit_model.predict_proba(X_v)[:,1]\n",
    "pred_val_2 = nb_model.predict_proba(X_v)[:,1]\n",
    "pred_val_3 = lgbm_model.predict_proba(X_v)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all predictions into a dataframe\n",
    "pred_val_1_df = np.reshape(pred_val_1, (pred_val_1.shape[0]))\n",
    "pred_val_2_df = np.reshape(pred_val_2, (pred_val_2.shape[0]))\n",
    "pred_val_3_df = np.reshape(pred_val_3, (pred_val_3.shape[0]))\n",
    "\n",
    "pred_val_df = np.reshape(y_v, (y_v.shape[0]))\n",
    "\n",
    "validation_df = pd.DataFrame({'val_1': pred_val_1_df, 'val_2': pred_val_2_df, 'val_3': pred_val_3_df, 'prediction': pred_val_df})\n",
    "validation_df.to_csv('validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_1</th>\n",
       "      <th>val_2</th>\n",
       "      <th>val_3</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.016429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122895</td>\n",
       "      <td>0.997764</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009584</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.016544</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006439</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.037953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.023346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      val_1     val_2     val_3  prediction\n",
       "0  0.001799  0.000002  0.016429           0\n",
       "1  0.122895  0.997764  0.272211           1\n",
       "2  0.009584  0.000371  0.016544           0\n",
       "3  0.006439  0.001017  0.037953           0\n",
       "4  0.002904  0.000003  0.023346           0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see predictions on the hold-out set from all three model and the ground thruth in the last column. We observe difference between them. E.g. model2 (NB) is very certain about prediction in row 2 while other two models are not. \n",
    "\n",
    "Idea of ensembling is to take many \"specialized models\" and find best score for them as a group of experts compared to individual predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take the average prediction from ensemble\n",
    "One of the simplest techinques to combine results is to take the average or weigheted average of all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_1</th>\n",
       "      <th>val_2</th>\n",
       "      <th>val_3</th>\n",
       "      <th>prediction</th>\n",
       "      <th>mean_pred</th>\n",
       "      <th>weight_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.016429</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.003724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122895</td>\n",
       "      <td>0.997764</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>1</td>\n",
       "      <td>0.464290</td>\n",
       "      <td>0.276523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009584</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.016544</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008833</td>\n",
       "      <td>0.009246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006439</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.037953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015136</td>\n",
       "      <td>0.010353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.023346</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>0.005535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      val_1     val_2     val_3  prediction  mean_pred  weight_pred\n",
       "0  0.001799  0.000002  0.016429           0   0.006076     0.003724\n",
       "1  0.122895  0.997764  0.272211           1   0.464290     0.276523\n",
       "2  0.009584  0.000371  0.016544           0   0.008833     0.009246\n",
       "3  0.006439  0.001017  0.037953           0   0.015136     0.010353\n",
       "4  0.002904  0.000003  0.023346           0   0.008751     0.005535"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df['mean_pred'] = validation_df.iloc[:, 0:3].agg(np.mean, axis=1)\n",
    "validation_df['weight_pred'] = validation_df.val_1 * 0.7 + validation_df.val_2 * 0.15 + validation_df.val_3 * 0.15\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.540 [Average Prediction]\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score: %0.3f [Average Prediction]\" % bestThresshold(validation_df.prediction, validation_df.mean_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.552 [Weighted Average]\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score: %0.3f [Weighted Average]\" % bestThresshold(validation_df.prediction, validation_df.weight_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy didn't work and we've got lower score than by using single best classifer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create simple Voting Classifier\n",
    "This approach uses majority rule, where the final prediction is one that has most votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = VotingClassifier(estimators=models, voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('Logistic Regression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=4000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)...0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0))],\n",
       "         flatten_transform=None, n_jobs=None, voting='soft', weights=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_preds = clf.predict_proba(X_v)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.546 \n"
     ]
    }
   ],
   "source": [
    "print (\"F1 Score: %0.3f \" % bestThresshold(y_v, ens_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, dissapoiting.. Result did not improve with ensemble compared to using simple Logistic Regression! \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "There must be smarter ways to create an ensamble but this is my first encounter with ensembling and I didn't have much time to improve..\n",
    "\n",
    "Some additional steps that may be taken to improve could be:\n",
    "\n",
    "1. Running the tuning for longer. In my case it took few hours to train forest classifiers on only 5 epoch! Bayesian Optimization requires high number of rounds to converge to optimal parameter.\n",
    "1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n",
    "1. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\n",
    "\n",
    "\n",
    "\n",
    "### Next...\n",
    "Since everyone in the competition were using neural networks I decided to test it. In next notebook we check the performance of a LSTM Neural Network which is proven to work well on this type of problems. \n",
    "\n",
    "Additionally with neural networks, we can use _word embeedings_ to efficiently represent words of each sentence is n-dimensional space. Using embeddings we will keep all words without stemming them, thus keeping natural meaning and algorithm will be able to infer similarities between words giving extra boost to performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
